{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpSV5XXh3Rq9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import statistics as st\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict  \n",
        "\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import RegexpTokenizer, word_tokenize, TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/appr_1_kmeans_cluster_labels.txt ."
      ],
      "metadata": {
        "id": "NkD-3Ux1CogK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/appr_1_cluster_labels.txt ."
      ],
      "metadata": {
        "id": "xQ5f8lmSR8TY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach with K means"
      ],
      "metadata": {
        "id": "zQM05pc4EBJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "appr_clusters_dict_1 = {}\n",
        "cluster_file = open('/content/drive/MyDrive/result_test_step_clustering/appr_1_kmeans_cluster_labels.txt')\n",
        "for line in cluster_file:\n",
        "    full_line = line.split()\n",
        "    cluster_id = int(full_line[0].replace('[', '').replace(']', '').replace(':', ''))\n",
        "    step_id_list = full_line[1].split(',')\n",
        "    for step_id in step_id_list:\n",
        "        appr_clusters_dict_1[int(float(step_id))] = cluster_id"
      ],
      "metadata": {
        "id": "MzN-sV-o3boU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of test steps which were clustered by the approach: \", len(appr_clusters_dict_1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3EhnAUE3ebs",
        "outputId": "e87efaa1-5bf7-4613-e282-4df2b365e32b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test steps which were clustered by the approach:  8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "appr_clusters_dict_2 = {}\n",
        "cluster_file = open('/content/drive/MyDrive/result_test_step_clustering/appr_1_cluster_labels.txt')\n",
        "for line in cluster_file:\n",
        "    full_line = line.split()\n",
        "    cluster_id = int(full_line[0].replace('[', '').replace(']', '').replace(':', ''))\n",
        "    step_id_list = full_line[1].split(',')\n",
        "    for step_id in step_id_list:\n",
        "        appr_clusters_dict_2[int(float(step_id))] = cluster_id"
      ],
      "metadata": {
        "id": "WQxYrIwnQgL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of test steps which were clustered by the approach: \", len(appr_clusters_dict_2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxXN3RIVQfzw",
        "outputId": "99b2520e-5072-48b4-d3df-345181dc36e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test steps which were clustered by the approach:  8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identify clusters using ensemble approach (majority voting)\n",
        "Load data and preprocess/clean it "
      ],
      "metadata": {
        "id": "ujEYvpBMKzrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data and preprocess it\n"
      ],
      "metadata": {
        "id": "d4qotXCeK5AM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_number_unique_words(df):\n",
        "    words_list = list()\n",
        "    test_steps = list(df[\"Steps\"])\n",
        "    for step in test_steps:\n",
        "        for word in step:\n",
        "            words_list.append(word)\n",
        "    number_unique_words = len(set(words_list))\n",
        "    return number_unique_words\n"
      ],
      "metadata": {
        "id": "34r7H-D0KqlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_word_frequency(df):\n",
        "    words_list = list()\n",
        "    test_steps = list(df[\"Steps\"])\n",
        "    for step in test_steps:\n",
        "        for word in step:\n",
        "            words_list.append(word)\n",
        "    unique_words_list = set(words_list)\n",
        "    word_occurrence_dict = {}\n",
        "    for each_word in unique_words_list:\n",
        "        word_occurrence_dict[each_word] = 0\n",
        "\n",
        "    for step in test_steps:\n",
        "        for word in step:\n",
        "            word_occurrence_dict[word] += 1\n",
        "            \n",
        "    ten_times_occurrence_words = list()\n",
        "    # get list of words that occur only once\n",
        "    for word, occurrence in word_occurrence_dict.items():\n",
        "        if occurrence < 2:\n",
        "            ten_times_occurrence_words.append(word)\n",
        "\n",
        "    return ten_times_occurrence_words\n"
      ],
      "metadata": {
        "id": "CkBs1mK_LRsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_problematic_words(df):\n",
        "    number_unique_words = get_number_unique_words(df)\n",
        "    print(\"Number of unique words across all test steps: \", number_unique_words)\n",
        "    \n",
        "   \n",
        "    problematic_words = open('word2vec_vocab_problematic.txt', 'r')\n",
        "    problematic_words_list = list()\n",
        "    for word in problematic_words:\n",
        "        problematic_words_list.append(word.lstrip().rstrip())\n",
        "    \n",
        "    for index, row in df.iterrows():\n",
        "        step = row[\"Steps\"]\n",
        "        df.loc[index][\"Steps\"] = [elem for elem in step if not elem in problematic_words_list]\n",
        "        \n",
        "    number_unique_words = get_number_unique_words(df)\n",
        "    print(\"Number of unique words across all test steps after removing problematic words: \", number_unique_words)\n"
      ],
      "metadata": {
        "id": "8bMRvdXxLT9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def fix_problematic_words(df):\n",
        "    number_unique_words = get_number_unique_words(df)\n",
        "    print(\"Number of unique words across all test steps: \", number_unique_words)\n",
        "    \n",
        " \n",
        "    problematic_words = open('word2vec_vocab_to_fix.txt', 'r')\n",
        "    problematic_words_dict = {}\n",
        "    for line in problematic_words:\n",
        "        full_line = line.split(':')\n",
        "        try:\n",
        "            problematic_words_dict[full_line[0]] = [x.replace('\\n', '') for x in full_line[1].split(',')]\n",
        "        except:\n",
        "            problematic_words_dict[full_line[0]] = full_line[1].replace('\\n', '')\n",
        "    \n",
        "    for index, row in df.iterrows():\n",
        "        step = row[\"Steps\"]\n",
        "        modified_step = list()\n",
        "        for word in step:\n",
        "            if word in problematic_words_dict:\n",
        "                modified_step.extend(problematic_words_dict[word])\n",
        "            else:\n",
        "                modified_step.append(word)\n",
        "        df.loc[index][\"Steps\"] = modified_step \n",
        "        \n",
        "    number_unique_words = get_number_unique_words(df)\n",
        "    print(\"Number of unique words across all test steps after fixing problematic words: \", number_unique_words)\n"
      ],
      "metadata": {
        "id": "GBF7IxXbLXmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "current_dir = os.getcwd() \n",
        "parent_dir = os.path.dirname(current_dir) + \"\\\\filtered_data\\\\\"\n",
        "xlsxfiles = [os.path.join(root, name)\n",
        "             for root, dirs, files in os.walk(parent_dir)\n",
        "             for name in files\n",
        "             if name.endswith((\".xlsx\"))]\n",
        "\n"
      ],
      "metadata": {
        "id": "Z3gQ5h-dLnnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "column_names = [\"Type\", \"Key\", \"Case_Name\", \"Step_ID\", \"Steps\"]\n",
        "test_steps_df = pd.DataFrame(columns = column_names)\n",
        "\n",
        "\n",
        "index_to_add = 0\n",
        "\n",
        "print(\"Reading input data...\")   \n",
        "\n",
        "test_file='/content/drive/MyDrive/result_test_step_clustering/test_cases.xlsx'\n",
        "test_data_df = pd.read_excel(test_file)\n",
        "for index, row in test_data_df.iterrows():\n",
        "        current_type = row[\"Type\"]\n",
        "        current_key = row[\"Key\"]\n",
        "        current_name = row[\"Case_Name\"]\n",
        "        current_step_id = row[\"Step_ID\"]\n",
        "        current_steps = row[\"Steps\"]\n",
        "        test_steps_df.loc[index_to_add] = [current_type, current_key, current_name, current_step_id, current_steps]\n",
        "        index_to_add += 1\n",
        "\n",
        "print(\"Done!\")\n",
        "print(\"Shape of data => \", test_steps_df.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7-VXvGQLsEO",
        "outputId": "f0d096db-9642-4d71-de3b-a64f2bd25ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading input data...\n",
            "Done!\n",
            "Shape of data =>  (369, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "G46ZMUL2OTrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDi8VIJaOWpM",
        "outputId": "8c5a7d45-308c-468b-a349-e6a82ab528fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_clean_data(test_steps_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4j8wXUsLu5V",
        "outputId": "f921d8c2-cde9-4fde-c1c3-f35b15da3c7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning test step field...\n",
            "Number of unique words across all test steps:  818\n",
            "Number of words that occurred less than 10 times in test steps:  384\n",
            "Dataset size after preprocessing:  (369, 5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/core/series.py:1056: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  cacher_needs_updating = self._check_is_chained_assignment_possible()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "step_id_text_tuple_list = list()\n",
        "test_steps_clustering_list = list()\n",
        "for index, row in test_steps_df.iterrows():\n",
        "    step_id = row[\"Step_ID\"]\n",
        "    step_text = row[\"Steps\"]\n",
        "    step_id_text_tuple_list.append((step_id,step_text))\n",
        "\n",
        "    temp_list = list()\n",
        "    if isinstance(row[\"Steps\"], list):\n",
        "        for elem in row[\"Steps\"]:\n",
        "            temp_list.append(elem)\n",
        "    else:\n",
        "        if isinstance(row[\"Steps\"], str):\n",
        "            temp_list.append(row[\"Steps\"])\n",
        "        \n",
        "   \n",
        "    test_steps_clustering_list.append(temp_list)\n",
        "    \n",
        "print(\"Length of list of tuples:\" , len(step_id_text_tuple_list))\n",
        "print(\"Length of list with test steps: \" , len(test_steps_clustering_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEyGXB0TN4pM",
        "outputId": "73963169-8b8e-456f-8daa-f11162a26656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of list of tuples: 369\n",
            "Length of list with test steps:  369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "index = 0\n",
        "steps_to_remove = list()\n",
        "for step in test_steps_clustering_list:\n",
        "    if len(step) == 0:\n",
        "        steps_to_remove.append(index)\n",
        "    index += 1\n",
        "\n",
        "step_id_text_tuple_list = [step_id_text_tuple_list[index] for index in range(len(step_id_text_tuple_list)) if not index in steps_to_remove]\n",
        "test_steps_clustering_list = [test_steps_clustering_list[index] for index in range(len(test_steps_clustering_list)) if not index in steps_to_remove]\n",
        "print(\"Length of list of tuples:\" , len(step_id_text_tuple_list))\n",
        "print(\"Length of list with test steps: \" , len(test_steps_clustering_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RIRh_9JPbzu",
        "outputId": "66b1cf90-7865-4d7c-ac86-b7a90321f116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of list of tuples: 369\n",
            "Length of list with test steps:  369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "clusters_list = []\n",
        "found_flag = [False] * len(test_steps_clustering_list)\n",
        "\n",
        "for i in range(len(test_steps_clustering_list)-1):\n",
        "    temp_set = set()\n",
        "    if not found_flag[i]:\n",
        "        temp_set.add(i)\n",
        "        found_flag[i] = True\n",
        "    else:\n",
        "        continue\n",
        "        \n",
        "    for j in range(i+1, len(test_steps_clustering_list)):\n",
        "        if found_flag[j]:\n",
        "            continue\n",
        "        else:\n",
        "            step_id_1 = step_id_text_tuple_list[i][0]\n",
        "            step_id_2 = step_id_text_tuple_list[j][0]\n",
        "            if ( (appr_clusters_dict_1[step_id_1] == appr_clusters_dict_1[step_id_2]) + (appr_clusters_dict_2[step_id_1] == appr_clusters_dict_2[step_id_2])) >= 3:  \n",
        "                temp_set.add(j)\n",
        "                found_flag[j] = True\n",
        "    clusters_list.append(temp_set)"
      ],
      "metadata": {
        "id": "FfMLCCCYPfM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(len(clusters_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoU1al8jPihe",
        "outputId": "e56a2d8b-d3ac-4d87-ed2e-2fea3f834c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "path_save_data = \"/content/ensemble_clustered_data.txt\"\n",
        "out_cluster_file = open(path_save_data, \"a\")\n",
        "cluster_id = 0\n",
        "\n",
        "for cluster in clusters_list:\n",
        "    for index in cluster: \n",
        "        str_to_save = \"[\" + str(cluster_id) + \"]:\\t\\t\" + test_steps_df.loc[index][\"Key\"] + \"\\t\\t\" + str(step_id_text_tuple_list[index][0]) + \"\\t\\t\" + str(test_steps_clustering_list[index]) + \"\\n\"\n",
        "        out_cluster_file.write(str_to_save)\n",
        "    cluster_id += 1\n",
        "out_cluster_file.close()"
      ],
      "metadata": {
        "id": "duhsM5jYP8W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "path_save_labels = \"/content/ensemble_cluster_labels.txt\"\n",
        "out_cluster_file = open(path_save_labels, \"a\")\n",
        "cluster_id = 0\n",
        "for cluster in clusters_list:\n",
        "    str_to_save = \"[\" + str(cluster_id) + \"]: \" + ','.join(str(step_id_text_tuple_list[x][0]) for x in list(cluster)) + \"\\n\"\n",
        "    out_cluster_file.write(str_to_save)\n",
        "    cluster_id += 1\n",
        "out_cluster_file.close()"
      ],
      "metadata": {
        "id": "qaK-qxKqX--E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "appr_ensemble_clusters_dict = {}\n",
        "cluster_id = 0\n",
        "for each_set in clusters_list:\n",
        "    for index in each_set:\n",
        "        step_id = step_id_text_tuple_list[index][0]\n",
        "        appr_ensemble_clusters_dict[int(step_id)] = cluster_id\n",
        "    cluster_id += 1"
      ],
      "metadata": {
        "id": "zKspS76DTYzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of test steps which were clustered by the approach: \", len(appr_ensemble_clusters_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNbWfouNTcIk",
        "outputId": "5cc28048-f1ae-4e03-c759-8d79f0b295ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test steps which were clustered by the approach:  8\n"
          ]
        }
      ]
    }
  ]
}