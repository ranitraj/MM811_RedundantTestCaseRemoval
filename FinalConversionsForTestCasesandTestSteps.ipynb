{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7E5PPwKx958",
        "outputId": "956a16a7-c72f-42d5-db02-dee0b354d36f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import statistics as st\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict  \n",
        "\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import RegexpTokenizer, word_tokenize, TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "approach_ensemble_dict = {}\n",
        "cluster_file = open('/content/drive/MyDrive/ensemble_cluster_labels_updated.txt')\n",
        "for line in cluster_file:\n",
        "    full_line = line.split()\n",
        "    cluster_id = int(full_line[0].replace('[', '').replace(']', '').replace(':', ''))\n",
        "    step_id_list = full_line[1].split(',')\n",
        "    for step_id in step_id_list:\n",
        "        approach_ensemble_dict[int(float(step_id))] = cluster_id\n"
      ],
      "metadata": {
        "id": "jWXKg7eRyNIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of test steps which were clustered by the approach: \", len(approach_ensemble_dict))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ov936c4FyYe8",
        "outputId": "3a3af321-ce58-4e4d-fda1-b2d87e08f71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test steps which were clustered by the approach:  8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_number_unique_words(df):\n",
        "    words_list = list()\n",
        "    test_steps = list(df[\"Steps\"])\n",
        "    for step in test_steps:\n",
        "        for word in step:\n",
        "            words_list.append(word)\n",
        "    number_unique_words = len(set(words_list))\n",
        "    return number_unique_words"
      ],
      "metadata": {
        "id": "Df9N2LudydTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_word_frequency(df):\n",
        "    words_list = list()\n",
        "    test_steps = list(df[\"Steps\"])\n",
        "    for step in test_steps:\n",
        "        for word in step:\n",
        "            words_list.append(word)\n",
        "    unique_words_list = set(words_list)\n",
        "    word_occurrence_dict = {}\n",
        "    for each_word in unique_words_list:\n",
        "        word_occurrence_dict[each_word] = 0\n",
        "\n",
        "    for step in test_steps:\n",
        "        for word in step:\n",
        "            word_occurrence_dict[word] += 1\n",
        "            \n",
        "    ten_times_occurrence_words = list()\n",
        "    \n",
        "    for word, occurrence in word_occurrence_dict.items():\n",
        "        if occurrence < 2:\n",
        "            ten_times_occurrence_words.append(word)\n",
        "\n",
        "    return ten_times_occurrence_words\n"
      ],
      "metadata": {
        "id": "cm0Gm0FzygIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_problematic_words(df):\n",
        "    number_unique_words = get_number_unique_words(df)\n",
        "    print(\"Number of unique words across all test steps: \", number_unique_words)\n",
        "    \n",
        " \n",
        "    problematic_words = open('word2vec_vocab_problematic.txt', 'r')\n",
        "    problematic_words_list = list()\n",
        "    for word in problematic_words:\n",
        "        problematic_words_list.append(word.lstrip().rstrip())\n",
        "    \n",
        "    for index, row in df.iterrows():\n",
        "        step = row[\"Steps\"]\n",
        "        df.loc[index][\"Steps\"] = [elem for elem in step if not elem in problematic_words_list]\n",
        "        \n",
        "    number_unique_words = get_number_unique_words(df)\n",
        "    print(\"Number of unique words across all test steps after removing problematic words: \", number_unique_words)\n",
        "\n"
      ],
      "metadata": {
        "id": "KhGmOHKayikK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def fix_problematic_words(df):\n",
        "    number_unique_words = get_number_unique_words(df)\n",
        "    print(\"Number of unique words across all test steps: \", number_unique_words)\n",
        "    \n",
        "    problematic_words = open('word2vec_vocab_to_fix.txt', 'r')\n",
        "    problematic_words_dict = {}\n",
        "    for line in problematic_words:\n",
        "        full_line = line.split(':')\n",
        "        try:\n",
        "            problematic_words_dict[full_line[0]] = [x.replace('\\n', '') for x in full_line[1].split(',')]\n",
        "        except:\n",
        "            problematic_words_dict[full_line[0]] = full_line[1].replace('\\n', '')\n",
        "    \n",
        "    for index, row in df.iterrows():\n",
        "        step = row[\"Steps\"]\n",
        "        modified_step = list()\n",
        "        for word in step:\n",
        "            if word in problematic_words_dict:\n",
        "                modified_step.extend(problematic_words_dict[word])\n",
        "            else:\n",
        "                modified_step.append(word)\n",
        "        df.loc[index][\"Steps\"] = modified_step \n",
        "        \n",
        "    number_unique_words = get_number_unique_words(df)\n",
        "    print(\"Number of unique words across all test steps after fixing problematic words: \", number_unique_words)\n"
      ],
      "metadata": {
        "id": "vSUzTlgwykQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "column_names = [\"Type\", \"Key\", \"Case_Name\", \"Step_ID\", \"Steps\"]\n",
        "test_steps_df = pd.DataFrame(columns = column_names)\n",
        "\n",
        "index_to_add = 0\n",
        "\n",
        "print(\"Reading input data...\")   \n",
        "test_file='/content/drive/MyDrive/result_test_step_clustering/test_cases.xlsx'\n",
        "    \n",
        "test_data_df = pd.read_excel(test_file)\n",
        "for index, row in test_data_df.iterrows():\n",
        "      current_type = row[\"Type\"]\n",
        "      current_key = row[\"Key\"]\n",
        "      current_name = row[\"Case_Name\"]\n",
        "      current_step_id = row[\"Step_ID\"]\n",
        "      current_steps = row[\"Steps\"]\n",
        "      test_steps_df.loc[index_to_add] = [current_type, current_key, current_name, current_step_id, current_steps]\n",
        "      index_to_add += 1\n",
        "\n",
        "print(\"Done!\")\n",
        "print(\"Shape of data => \", test_steps_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUxwOFsFzQEw",
        "outputId": "8a6b7c62-3fc4-4c3b-f45c-343330a687ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading input data...\n",
            "Done!\n",
            "Shape of data =>  (369, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "preprocess_clean_data(test_steps_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6e9p9ngzmBe",
        "outputId": "286200d3-41f8-4e06-b26a-d9cd9c1b2f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning test step field...\n",
            "Number of unique words across all test steps:  818\n",
            "Number of unique words in test steps after stopword removal:  742\n",
            "Number of unique words in test steps after lemmatization:  683\n",
            "Number of words that occurred less than 10 times in test steps:  331\n",
            "Dataset size after preprocessing:  (369, 5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/core/series.py:1056: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  cacher_needs_updating = self._check_is_chained_assignment_possible()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "step_id_text_tuple_list = list()\n",
        "test_steps_clustering_list = list()\n",
        "for index, row in test_steps_df.iterrows():\n",
        "    step_id = row[\"Step_ID\"]\n",
        "    step_text = row[\"Steps\"]\n",
        "    step_id_text_tuple_list.append((step_id,step_text))\n",
        "\n",
        "    temp_list = list()\n",
        "    if isinstance(row[\"Steps\"], list):\n",
        "        for elem in row[\"Steps\"]:\n",
        "            temp_list.append(elem)\n",
        "    else:\n",
        "        if isinstance(row[\"Steps\"], str):\n",
        "            temp_list.append(row[\"Steps\"])\n",
        "        \n",
        "    \n",
        "    test_steps_clustering_list.append(temp_list)\n",
        "    \n",
        "print(\"Length of list of tuples:\" , len(step_id_text_tuple_list))\n",
        "print(\"Length of list with test steps: \" , len(test_steps_clustering_list))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F53ld4Ibzo7z",
        "outputId": "d5faf790-c48a-4b4c-aac2-79de37b07ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of list of tuples: 369\n",
            "Length of list with test steps:  369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "index = 0\n",
        "steps_to_remove = list()\n",
        "for step in test_steps_clustering_list:\n",
        "    if len(step) == 0:\n",
        "        steps_to_remove.append(index)\n",
        "    index += 1\n",
        "\n",
        "step_id_text_tuple_list = [step_id_text_tuple_list[index] for index in range(len(step_id_text_tuple_list)) if not index in steps_to_remove]\n",
        "test_steps_clustering_list = [test_steps_clustering_list[index] for index in range(len(test_steps_clustering_list)) if not index in steps_to_remove]\n",
        "print(\"Length of list of tuples:\" , len(step_id_text_tuple_list))\n",
        "print(\"Length of list with test steps: \" , len(test_steps_clustering_list))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVE-t1Hqzvhl",
        "outputId": "fc8cc364-8ce4-4b0f-f72f-c3ee5e95984b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of list of tuples: 369\n",
            "Length of list with test steps:  369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_case_steps_dict = {}\n",
        "for index, row in test_steps_df.iterrows():\n",
        "    test_case_key = row['Key']\n",
        "    test_step_id = row['Step_ID']\n",
        "    test_steps = row['Steps']\n",
        "    if len(test_steps) == 0:\n",
        "        continue\n",
        "        \n",
        "    if test_case_key in test_case_steps_dict:\n",
        "        existing_list = test_case_steps_dict[test_case_key]\n",
        "        existing_list.append(test_steps)\n",
        "        test_case_steps_dict[test_case_key] = existing_list\n",
        "    else:\n",
        "        test_case_steps_dict[test_case_key] = [test_steps]\n",
        "print(\"Number of test cases: \", len(test_case_steps_dict))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt7H7Hw6zyAE",
        "outputId": "1c83aa8f-5a3c-4e74-ade2-9ee34b551f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test cases:  126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_case_steps_keys_list = list(test_case_steps_dict.keys())\n"
      ],
      "metadata": {
        "id": "ynTEpgMLz0Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_test_case_tuples = list()\n",
        "for i in range(len(test_case_steps_keys_list)-1):\n",
        "    for j in range(i+1, len(test_case_steps_keys_list)):\n",
        "        case_key_1 = test_case_steps_keys_list[i]\n",
        "        case_key_2 = test_case_steps_keys_list[j]\n",
        "        step_list_1 = test_case_steps_dict[case_key_1]\n",
        "        step_list_2 = test_case_steps_dict[case_key_2]\n",
        "        step_list_1 = [tuple(x) for x in step_list_1]\n",
        "        step_list_2 = [tuple(x) for x in step_list_2]\n",
        "        if set(step_list_1) == set(step_list_2):\n",
        "            duplicate_test_case_tuples.append((i,j))\n"
      ],
      "metadata": {
        "id": "yXkwSKABz19V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "duplicate_test_cases_list = list()\n",
        "for test_case_tuple in duplicate_test_case_tuples:\n",
        "    index_1 = test_case_tuple[0]\n",
        "    index_2 = test_case_tuple[1]\n",
        "    found = False\n",
        "    for test_case_set in duplicate_test_cases_list:\n",
        "        if (index_1 in test_case_set) or (index_2 in test_case_set):\n",
        "            test_case_set.add(index_1)\n",
        "            test_case_set.add(index_2)\n",
        "            found = True\n",
        "            break\n",
        "    if not found:\n",
        "        temp_set = set()\n",
        "        temp_set.add(index_1)\n",
        "        temp_set.add(index_2)\n",
        "        duplicate_test_cases_list.append(temp_set)\n",
        "print(\"Number of groups of similar test cases: \", len(duplicate_test_cases_list))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpQsK-Ocz4DP",
        "outputId": "9a715b7c-df19-4a7b-8a2c-da4179459db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of groups of similar test cases:  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_1_dict = {}\n",
        "cluster_id = 0\n",
        "indices_of_similar_cases = list()\n",
        "for each_set in duplicate_test_cases_list:\n",
        "    for elem in each_set:\n",
        "        indices_of_similar_cases.append(elem)\n",
        "        case_key = test_case_steps_keys_list[elem]\n",
        "        baseline_1_dict[case_key] = cluster_id\n",
        "    cluster_id += 1\n",
        "for elem in range(len(test_case_steps_keys_list)):\n",
        "    if elem not in indices_of_similar_cases:\n",
        "        case_key = test_case_steps_keys_list[elem]\n",
        "        baseline_1_dict[case_key] = cluster_id\n",
        "        cluster_id += 1\n"
      ],
      "metadata": {
        "id": "NUiC0eMuz5sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of test cases that have at least another similar case: \", len(indices_of_similar_cases))\n",
        "print(\"Number of test cases that do NOT have any similar case: \", ( len(test_case_steps_dict) - len(indices_of_similar_cases) ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBtgY5Jbz8b7",
        "outputId": "f3b9859b-fc97-4a0a-b78c-512e4f3db558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test cases that have at least another similar case:  9\n",
            "Number of test cases that do NOT have any similar case:  117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = '/content/baseline_1_similar_test_cases.txt'\n",
        "output_file = open(file_name, 'w')\n"
      ],
      "metadata": {
        "id": "KSjaTGACz-sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "for key in baseline_1_dict:\n",
        "    output_file.write(key + \":\" + str(baseline_1_dict[key]) + \"\\n\")\n",
        "output_file.close()"
      ],
      "metadata": {
        "id": "0h2N-kqd0EkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_cases_list = list()\n",
        "cases_added = list()\n",
        "\n",
        "for index, row in test_steps_df.iterrows():\n",
        "    case_key = row[\"Key\"]\n",
        "    case_name = row[\"Case_Name\"]\n",
        "    step_text = row[\"Steps\"]\n",
        "    \n",
        "    if len(step_text) == 0:\n",
        "        continue\n",
        "        \n",
        "    if case_key not in cases_added:\n",
        "        test_cases_list.append((case_key,case_name))\n",
        "        cases_added.append(case_key)\n",
        "    \n",
        "print(\"Length of list with test cases: \" , len(test_cases_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAkpz6Hw0Uc_",
        "outputId": "e6c3b30d-a329-408e-9b22-a56d36d1abb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of list with test cases:  126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similar_test_case_tuples = list()\n",
        "for i in range(len(test_cases_list)-1):\n",
        "    for j in range(i+1, len(test_cases_list)):\n",
        "        tuple_1 = test_cases_list[i]\n",
        "        tuple_2 = test_cases_list[j]\n",
        "        \n",
        "        case_key_1 = tuple_1[0]\n",
        "        case_name_1 = tuple_1[1]\n",
        "        case_key_2 = tuple_2[0]\n",
        "        case_name_2 = tuple_2[1]\n",
        "\n",
        "        if case_name_1 == case_name_2:\n",
        "            similar_test_case_tuples.append((i,j))"
      ],
      "metadata": {
        "id": "I5mB4vcG8Xx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "similar_test_cases_list = list()\n",
        "for test_case_tuple in similar_test_case_tuples:\n",
        "    index_1 = test_case_tuple[0]\n",
        "    index_2 = test_case_tuple[1]\n",
        "    found = False\n",
        "    for test_case_set in similar_test_cases_list:\n",
        "        if (index_1 in test_case_set) or (index_2 in test_case_set):\n",
        "            test_case_set.add(index_1)\n",
        "            test_case_set.add(index_2)\n",
        "            found = True\n",
        "            break\n",
        "    if not found:\n",
        "        temp_set = set()\n",
        "        temp_set.add(index_1)\n",
        "        temp_set.add(index_2)\n",
        "        similar_test_cases_list.append(temp_set)\n",
        "print(\"Number of groups of similar test cases: \", len(similar_test_cases_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yik0doM98bvR",
        "outputId": "4c86f3de-99b4-4e5c-804a-1374201b8283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of groups of similar test cases:  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_2_dict = {}\n",
        "cluster_id = 0\n",
        "indices_of_similar_cases = list()\n",
        "for each_set in similar_test_cases_list:\n",
        "    for elem in each_set:\n",
        "        indices_of_similar_cases.append(elem)\n",
        "        case_key = test_cases_list[elem][0]\n",
        "        baseline_2_dict[case_key] = cluster_id\n",
        "    cluster_id += 1\n",
        "for elem in range(len(test_cases_list)):\n",
        "    if elem not in indices_of_similar_cases:\n",
        "        case_key = test_cases_list[elem][0]\n",
        "        baseline_2_dict[case_key] = cluster_id\n",
        "        cluster_id += 1\n"
      ],
      "metadata": {
        "id": "y16-e0Ll8fUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of test cases that have at least another similar case: \", len(indices_of_similar_cases))\n",
        "print(\"Number of test cases that do NOT have any similar case: \", ( len(test_cases_list) - len(indices_of_similar_cases) ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "972rfuXj8jOo",
        "outputId": "282f2f84-3536-4987-a318-3b850f6aadbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test cases that have at least another similar case:  8\n",
            "Number of test cases that do NOT have any similar case:  118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = '/content/print_similar_test_cases.txt'\n",
        "output_file = open(file_name, 'w')\n"
      ],
      "metadata": {
        "id": "DFj8vYK-8mp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "for key in baseline_2_dict:\n",
        "    output_file.write(key + \":\" + str(baseline_2_dict[key]) + \"\\n\")\n",
        "output_file.close()\n"
      ],
      "metadata": {
        "id": "IUgXhqQB8wBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HcPjME0r8zZH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}