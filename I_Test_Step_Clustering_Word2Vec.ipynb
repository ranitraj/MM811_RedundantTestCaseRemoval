{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xcdpSeMcU7K"
   },
   "source": [
    "# STEP-1: Test Steps clustering with Word2Vec\n",
    "\n",
    "1. Text embedding technique: Word2Vec\n",
    "2. Text similarity: Word Mover's Distance (WMD)\n",
    "3. Clustering: K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUVgajTCbfrO",
    "outputId": "f389aef5-bded-4510-e8c1-f61a5d7c7bb3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ranitrajganguly/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ranitrajganguly/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/ranitrajganguly/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics as st\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For word frequency\n",
    "from collections import defaultdict\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from pyemd import emd\n",
    "from gensim.similarities import WmdSimilarity\n",
    "from gensim.models import Word2Vec, Phrases, KeyedVectors\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize, TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# To be used with hierarchical clustering\n",
    "from joblib import Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYDnZOFLeTaT"
   },
   "source": [
    "# Data preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'test_cases.xlsx'\n",
    "\n",
    "TYPE = \"Type\"\n",
    "KEY = \"Key\"\n",
    "CASE_NAME = \"Case_Name\"\n",
    "STEPS = \"Steps\"\n",
    "STEP_ID = \"Step_ID\"\n",
    "LIST_COLUMN = [\"Type\", \"Key\", \"Case_Name\", \"Step_ID\", \"Steps\"]\n",
    "\n",
    "VECTOR_DIMENSION = 200\n",
    "WORKER_COUNT = 4\n",
    "WORD_FREQUENCY_THRESHOLD = 2\n",
    "CONTEXT_WINDOW_SIZE = 2\n",
    "DOWN_SAMPLING = 0.001\n",
    "\n",
    "K_MEANS_CLUSTER_COUNT = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HOlBiJxGeD2n"
   },
   "outputs": [],
   "source": [
    "def get_unique_word_count(data, column_name):\n",
    "    \"\"\"\n",
    "    Calculates the unique wordcount in the dataset from a particular column\n",
    "\n",
    "    :param data: input data\n",
    "    :param column_name: name of the column from which unique word count is calculated\n",
    "    :return: unique word count\n",
    "    \"\"\"\n",
    "    list_words = list()\n",
    "    column = list(data[column_name])\n",
    "    for cur_step in column:\n",
    "        for cur_word in cur_step:\n",
    "            list_words.append(cur_word)\n",
    "    return len(set(list_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TQXMbGp-eXIA"
   },
   "outputs": [],
   "source": [
    "def get_word_frequency(data, column_name):\n",
    "    \"\"\"\n",
    "    Returns the list of words that have a frequency less than two in the dataset\n",
    "\n",
    "    :param data: input data\n",
    "    :param column_name: name of the column from which word frequency is calculated\n",
    "    :return: list of words\n",
    "    \"\"\"\n",
    "    list_words = list()\n",
    "    column = list(data[column_name])\n",
    "    for cur_step in column:\n",
    "        for cur_word in cur_step:\n",
    "            list_words.append(cur_word)\n",
    "    unique_words_list = set(list_words)\n",
    "\n",
    "    dict_word_frequency = {}\n",
    "    for cur_word in unique_words_list:\n",
    "        # Initialize count of all words to 0\n",
    "        dict_word_frequency[cur_word] = 0\n",
    "\n",
    "    for cur_step in column:\n",
    "        # Compute frequency of each word\n",
    "        for cur_word in cur_step:\n",
    "            dict_word_frequency[cur_word] += 1\n",
    "\n",
    "    final_list = list()\n",
    "    # List of words that occur only once\n",
    "    for word, frequency in dict_word_frequency.items():\n",
    "        if frequency < 2:\n",
    "            final_list.append(word)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Tzo0Mm82eaDK"
   },
   "outputs": [],
   "source": [
    "def read_input_data(data):\n",
    "    \"\"\"\n",
    "    Method to load input data and iterate through it\n",
    "\n",
    "    :param data: training data\n",
    "    \"\"\"\n",
    "    print(\"Reading input data\")\n",
    "    cur_index = 0\n",
    "    for index, row in data.iterrows():\n",
    "        current_type = row[TYPE]\n",
    "        current_key = row[KEY]\n",
    "        current_name = row[CASE_NAME]\n",
    "        current_step_id = row[STEP_ID]\n",
    "        current_steps = row[STEPS]\n",
    "\n",
    "        data.loc[cur_index] = [current_type, current_key, current_name, current_step_id, current_steps]\n",
    "        cur_index += 1\n",
    "    print(\"Shape of data = \", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hOQodzE_ecbW"
   },
   "outputs": [],
   "source": [
    "def clean_dataset(data):\n",
    "    \"\"\"\n",
    "    Cleans the input dataset for the test-steps and test-case columns\n",
    "\n",
    "    :param data: input data\n",
    "    :return: cleaned dataset\n",
    "    \"\"\"\n",
    "    print(\"Size of dataset before preprocessing = \", data.shape)\n",
    "\n",
    "    # Replace URL, paths, HTML tags and convert to lower-case\n",
    "    data[STEPS] = data[STEPS].apply(lambda x: re.sub(r'http\\S+', 'URL', x))\n",
    "    data[CASE_NAME] = data[CASE_NAME].apply(lambda x: re.sub(r'http\\S+', 'URL', x))\n",
    "    data[STEPS] = data[STEPS].apply(lambda x: re.sub(r'/[\\w-]*', '', x))\n",
    "    data[CASE_NAME] = data[CASE_NAME].apply(lambda x: re.sub(r'/[\\w-]*', '', x))\n",
    "    data[STEPS] = data[STEPS].apply(lambda x: re.sub(r'\\{[^)]*}', '', x))\n",
    "    data[CASE_NAME] = data[CASE_NAME].apply(lambda x: re.sub(r'\\{[^)]*}', '', x))\n",
    "    data[STEPS] = data[STEPS].apply(lambda x: x.lower())\n",
    "    data[CASE_NAME] = data[CASE_NAME].apply(lambda x: x.lower())\n",
    "\n",
    "    # Remove digits, punctuations and extra-spaces\n",
    "    data[STEPS] = data[STEPS].apply(lambda x: re.sub('\\w*\\d\\w*', '', x))\n",
    "    data[CASE_NAME] = data[CASE_NAME].apply(lambda x: re.sub('\\w*\\d\\w*', '', x))\n",
    "    data[STEPS] = data[STEPS].apply(\n",
    "        lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x))\n",
    "    data[CASE_NAME] = data[CASE_NAME].apply(\n",
    "        lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x))\n",
    "    data[STEPS] = data[STEPS].apply(lambda x: re.sub(' +', ' ', x))\n",
    "    data[CASE_NAME] = data[CASE_NAME].apply(lambda x: re.sub(' +', ' ', x))\n",
    "\n",
    "    # Tokenization\n",
    "    data[STEPS] = data[STEPS].apply(lambda x: TweetTokenizer().tokenize(x))\n",
    "    data[CASE_NAME] = data[CASE_NAME].apply(lambda x: TweetTokenizer().tokenize(x))\n",
    "    unique_word_count_steps = get_unique_word_count(data, STEPS)\n",
    "    unique_word_count_case = get_unique_word_count(data, CASE_NAME)\n",
    "    print(f\"Number of unique words across Test-cases = {unique_word_count_case} \"\n",
    "          f\"& Test-Steps = {unique_word_count_steps} after Tokenization\")\n",
    "\n",
    "    # Stopword removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    data[STEPS] = data[STEPS].apply(lambda x: [w for w in x if not w in stop_words])\n",
    "    data[CASE_NAME] = data[CASE_NAME].apply(lambda x: [w for w in x if not w in stop_words])\n",
    "    unique_word_count_steps = get_unique_word_count(data, STEPS)\n",
    "    unique_word_count_case = get_unique_word_count(data, CASE_NAME)\n",
    "    print(f\"Number of unique words across Test-cases = {unique_word_count_case} \"\n",
    "          f\"& Test-Steps = {unique_word_count_steps} after Stopword Removal\")\n",
    "\n",
    "    # Lemmatization for test case-names\n",
    "    word_net_lemmatizer = WordNetLemmatizer()\n",
    "    data[STEPS] = data[STEPS].apply(lambda x: [word_net_lemmatizer.lemmatize(w) for w in x])\n",
    "    data[CASE_NAME] = data[CASE_NAME].apply(lambda x: [word_net_lemmatizer.lemmatize(w) for w in x])\n",
    "\n",
    "    # Remove words that occur a certain number of times\n",
    "    word_frequency_threshold_steps = get_word_frequency(data, STEPS)\n",
    "    word_frequency_threshold_case = get_word_frequency(data, CASE_NAME)\n",
    "    print(f\"Number of words that occurred only once in Test-Cases = {len(word_frequency_threshold_case)}\"\n",
    "          f\"& Test-Steps = {len(word_frequency_threshold_steps)}\")\n",
    "\n",
    "    # List of words to be removed in Test-Steps\n",
    "    for index, row in data.iterrows():\n",
    "        current_test_name = row[STEPS]\n",
    "        list_words_to_remove_steps = list()\n",
    "        for word in current_test_name:\n",
    "            if word in word_frequency_threshold_case:\n",
    "                list_words_to_remove_steps.append(word)\n",
    "        data.loc[index][STEPS] = [elem for elem in current_test_name if\n",
    "                                            not elem in list_words_to_remove_steps]\n",
    "\n",
    "    # List of words to be removed in Test-Case\n",
    "    for index, row in data.iterrows():\n",
    "        current_test_name = row[CASE_NAME]\n",
    "        list_words_to_remove_case = list()\n",
    "        for word in current_test_name:\n",
    "            if word in word_frequency_threshold_case:\n",
    "                list_words_to_remove_case.append(word)\n",
    "        data.loc[index][CASE_NAME] = [elem for elem in current_test_name if\n",
    "                                                not elem in list_words_to_remove_case]\n",
    "\n",
    "    # Remove instances with empty names\n",
    "    data = data.loc[data[STEPS] != '']\n",
    "    data = data.loc[data[CASE_NAME] != '']\n",
    "\n",
    "    unique_word_count_steps = get_unique_word_count(data, STEPS)\n",
    "    unique_word_count_case = get_unique_word_count(data, CASE_NAME)\n",
    "    print(f\"Unique word count in Test-Case = {unique_word_count_case} \"\n",
    "          f\"& Unique word count in Test-Steps = {unique_word_count_steps}\")\n",
    "\n",
    "    print(\"Size of dataset after preprocessing = \", data.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rnycYSViem2U"
   },
   "outputs": [],
   "source": [
    "def return_training_list(data):\n",
    "    \"\"\"\n",
    "    Returns the necessary fields to train the word2vec word embedding model i.e 'type', 'name', 'steps'\n",
    "\n",
    "    :param data: preprocessed data\n",
    "    :return: training data\n",
    "    \"\"\"\n",
    "    print(\"Returning Training list\")\n",
    "    training_list = list()\n",
    "    for index, row in data.iterrows():\n",
    "        temp_list = list()\n",
    "\n",
    "        if not pd.isnull(row[TYPE]):\n",
    "            temp_list.append(str(row[TYPE]))\n",
    "\n",
    "        if isinstance(row[CASE_NAME], list):\n",
    "            for elem in row[CASE_NAME]:\n",
    "                temp_list.append(elem)\n",
    "        else:\n",
    "            if isinstance(row[CASE_NAME], str):\n",
    "                temp_list.append(row[CASE_NAME])\n",
    "\n",
    "        if isinstance(row[STEPS], list):\n",
    "            for elem in row[STEPS]:\n",
    "                temp_list.append(elem)\n",
    "        else:\n",
    "            if isinstance(row[STEPS], str):\n",
    "                temp_list.append(row[STEPS])\n",
    "\n",
    "        # List of lists of tokens\n",
    "        training_list.append(temp_list)\n",
    "    print(f\"Length of list with training data = {len(training_list)}\")\n",
    "    return training_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "be_W6vjeev4B",
    "outputId": "d6ca27f4-0366-4706-89bd-41a94c24b2c0"
   },
   "source": [
    "# Training Word2Vec Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "gYjoAJSOe-8O"
   },
   "outputs": [],
   "source": [
    "def train_word2vec_model(data, pre_trained_model, pre_trained_model_path, vector_size, workers, word_freq_threshold,\n",
    "                         context_window_size, down_sampling):\n",
    "    \"\"\"\n",
    "    Trains and adds the new corpus into the pretrained word2vec model\n",
    "\n",
    "    :param data: preprocessed data\n",
    "    :param pre_trained_model: model\n",
    "    :param pre_trained_model_path: model path\n",
    "    :param vector_size: vector size dimension\n",
    "    :param workers: number of workers\n",
    "    :param word_freq_threshold: maximum frequency limit for a word\n",
    "    :param context_window_size: context size of CBOW\n",
    "    :param down_sampling: down-sampling value\n",
    "    :return: trained model\n",
    "    \"\"\"\n",
    "    print(\"Training word2vec model....\")\n",
    "    list_word_vector_median = list()\n",
    "\n",
    "    # Initialize model\n",
    "    my_model = Word2Vec(\n",
    "        vector_size=vector_size,\n",
    "        workers=workers,\n",
    "        min_count=word_freq_threshold,\n",
    "        window=context_window_size,\n",
    "        sample=down_sampling\n",
    "    )\n",
    "\n",
    "    # Build model and count total number of examples\n",
    "    my_model.build_vocab(data)\n",
    "    total_examples = my_model.corpus_count\n",
    "    all_words = list(my_model.wv.index_to_key)\n",
    "    for cur_word in all_words:\n",
    "        my_model.wv[cur_word] = np.zeros(VECTOR_DIMENSION)\n",
    "\n",
    "    # Update vocabulary with our corpus\n",
    "    my_model.build_vocab([list(pre_trained_model.index_to_key)], update=True)\n",
    "    my_model.wv.vectors_lockf = np.ones(len(my_model.wv))\n",
    "    my_model.wv.intersect_word2vec_format(pre_trained_model_path, binary=True, lockf=1.0)\n",
    "\n",
    "    # Get Mean & Standard-Deviation of initialized word vectors\n",
    "    for cur_word in all_words:\n",
    "        if any(my_model.wv[cur_word] != 0):\n",
    "            list_word_vector_median.append(np.median(my_model.wv[cur_word]))\n",
    "\n",
    "    # Initialize Mean & Standard-Deviation for normal distributions of medians\n",
    "    mu = np.mean(list_word_vector_median)\n",
    "    sigma = np.std(list_word_vector_median)\n",
    "\n",
    "    # Initialize the remaining word vectors that are not present in the pre-trained word2vec model\n",
    "    for cur_word in all_words:\n",
    "        if all(my_model.wv[cur_word] == 0):\n",
    "            new_word_vector = np.random.normal(mu, sigma, 200)\n",
    "            my_model.wv[cur_word] = new_word_vector\n",
    "\n",
    "    # Train the model\n",
    "    my_model.train(data, total_examples=total_examples, epochs=25)\n",
    "    return my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z56ag3bYfFc_",
    "outputId": "89c72794-cca2-4106-bd15-45a58d498f2b"
   },
   "outputs": [],
   "source": [
    "def return_data_tuple(data):\n",
    "    \"\"\"\n",
    "    Returns tuples with step_id, step which is used to retrieve the step_id after clustering\n",
    "\n",
    "    :param data: data\n",
    "    :return: list_tuple_step_id, list_test_step_clustering\n",
    "    \"\"\"\n",
    "    print(\"Returning data tuple\")\n",
    "    list_tuple_step_id = list()\n",
    "    list_test_step_clustering = list()\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        step_id = row[STEP_ID]\n",
    "        step = row[STEPS]\n",
    "        list_tuple_step_id.append((step_id, step))\n",
    "\n",
    "        temp_list = list()\n",
    "        if isinstance(row[STEPS], list):\n",
    "            for cur_element in row[STEPS]:\n",
    "                temp_list.append(cur_element)\n",
    "        else:\n",
    "            if isinstance(row[STEPS], str):\n",
    "                temp_list.append(row[STEPS])\n",
    "        list_test_step_clustering.append(temp_list)\n",
    "\n",
    "    print(f\"First Tuple = {list_tuple_step_id[0]}\")\n",
    "    return list_tuple_step_id, list_test_step_clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Similarity using Word-Mover's Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d29u0X3NfMsX",
    "outputId": "788f885e-fa95-4e93-8696-06a1584cac6d"
   },
   "outputs": [],
   "source": [
    "def initialize_similarity_matrix(list_test_step_clustering):\n",
    "    \"\"\"\n",
    "    Create and returns an empty matrix with rows and columns equal to the shape of 'list_test_step_clustering'\n",
    "\n",
    "    :param list_test_step_clustering: clustering steps list\n",
    "    :return: matrix_similarity_distance\n",
    "    \"\"\"\n",
    "    print(\"Initializing Similarity Matrix\")\n",
    "    rows = columns = len(list_test_step_clustering)\n",
    "    return np.zeros((rows, columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Gox-G0sfiMu",
    "outputId": "becb4777-bd3e-4aca-b11a-f96f4249b272"
   },
   "outputs": [],
   "source": [
    "def compute_and_save_similarity_distance(model, list_test_step_clustering, matrix_similarity_distance):\n",
    "    \"\"\"\n",
    "    Computes the similarity distance between the rows and columns of list_test_step_clustering\n",
    "    and saves the result in a .txt file\n",
    "\n",
    "    :param model: word2vec model\n",
    "    :param list_test_step_clustering: clustering steps list\n",
    "    :param matrix_similarity_distance: empty matrix\n",
    "    :return: matrix_similarity_distance with similarities\n",
    "    \"\"\"\n",
    "    print(\"Computing Similarity using WMD and filling Similarity Matrix\")\n",
    "    rows = columns = len(list_test_step_clustering)\n",
    "\n",
    "    for cur_row in range(rows):\n",
    "        for cur_column in range(cur_row, columns):\n",
    "            similarity_distance = model.wv.wmdistance(list_test_step_clustering[cur_row],\n",
    "                                                      list_test_step_clustering[cur_column])\n",
    "            # Upper limit\n",
    "            if similarity_distance > 800:\n",
    "                similarity_distance = 800\n",
    "            matrix_similarity_distance[cur_row, cur_column] = matrix_similarity_distance[\n",
    "                cur_column, cur_row] = similarity_distance\n",
    "\n",
    "    # Save the similarity matrix\n",
    "    path_save_matrix_similarity = 'matrix_similarity.txt'\n",
    "    np.savetxt(path_save_matrix_similarity, matrix_similarity_distance)\n",
    "\n",
    "    return matrix_similarity_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "hAgwtjYNgTxx",
    "outputId": "b733b84a-2be7-4430-9108-bc196a242c79"
   },
   "outputs": [],
   "source": [
    "def compute_average_test_step(test_step, model, pre_trained_model):\n",
    "    \"\"\"\n",
    "    Computes the average word vector of the test step\n",
    "\n",
    "    :param test_step: current test-step\n",
    "    :param model: trained word2vec model\n",
    "    :param pre_trained_model: pre-trained word2vec model\n",
    "    :return: average\n",
    "    \"\"\"\n",
    "    list_word_vectors = list()\n",
    "    count_word = 0\n",
    "    for word in test_step:\n",
    "        try:\n",
    "            list_word_vectors.append(model[word])\n",
    "        except:\n",
    "            try:\n",
    "                list_word_vectors.append(pre_trained_model[word])\n",
    "            except:\n",
    "                return np.zeros(200)\n",
    "        count_word += 1\n",
    "    sum_vectors = sum(list_word_vectors)\n",
    "    average = sum_vectors / count_word\n",
    "    return average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering using K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mXSBvBhihtxL",
    "outputId": "02cb8ee6-6920-4064-8624-22b51fe67353"
   },
   "outputs": [],
   "source": [
    "def perform_clustering_and_save(model, pre_trained_model, data, list_tuple_step_id, list_test_step_clustering):\n",
    "    \"\"\"\n",
    "    Performs k-means clustering on the test_step and step_id and saves the clustered results in .txt files\n",
    "\n",
    "    :param model: trained word2vec model\n",
    "    :param pre_trained_model: pre-trained word2vec model\n",
    "    :param data: processed data\n",
    "    :param list_tuple_step_id: tuple of step-id\n",
    "    :param list_test_step_clustering: clustered list\n",
    "    :return: dict_clusters: dictionary of the clusters\n",
    "    \"\"\"\n",
    "    labels_list_kmeans = list()\n",
    "\n",
    "    # Compute average word vector to be used in k-means\n",
    "    avg_word_sentence_vectors = list()\n",
    "    for cur_test_step in list_test_step_clustering:\n",
    "        if len(cur_test_step) > 0:\n",
    "            avg_word_sentence_vectors.append(\n",
    "                compute_average_test_step(\n",
    "                    cur_test_step,\n",
    "                    model,\n",
    "                    pre_trained_model\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Initialize K-means\n",
    "    k_means = KMeans(\n",
    "        n_clusters=K_MEANS_CLUSTER_COUNT,\n",
    "        init='k-means++',\n",
    "        max_iter=500\n",
    "    )\n",
    "    k_means.fit(avg_word_sentence_vectors)\n",
    "    labels = k_means.labels_\n",
    "    labels_list_kmeans.append(labels)\n",
    "\n",
    "    dict_clusters = {}\n",
    "    for cur_label in set(labels):\n",
    "        label_indices = np.where(labels == cur_label)[0].tolist()\n",
    "        for cur_index in label_indices:\n",
    "            dict_clusters[int(list_tuple_step_id[cur_index][0])] = cur_label\n",
    "\n",
    "    print(f\"Number of clusters = {k_means.n_clusters} & Number of labels = {k_means.labels_}\")\n",
    "    save_clusters(k_means.labels_, list_tuple_step_id, list_test_step_clustering, data)\n",
    "    save_cluster_labels(k_means.labels_, list_tuple_step_id)\n",
    "\n",
    "    return dict_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "S32b7j_Oh3pm"
   },
   "outputs": [],
   "source": [
    "def save_clusters(labels, list_tuple_step_id, list_test_step_clustering, data):\n",
    "    \"\"\"\n",
    "    Saves clusters in a text-file\n",
    "\n",
    "    :param labels: k-means labels\n",
    "    :param list_tuple_step_id: tuple of step-id\n",
    "    :param list_test_step_clustering: clustered list\n",
    "    :param data: processed data\n",
    "    \"\"\"\n",
    "    path = \"clustered_data_kmeans.txt\"\n",
    "    output = open(path, \"a\")\n",
    "    for label in set(labels):\n",
    "        indices_label = np.where(labels == label)[0].tolist()\n",
    "        for index in indices_label:\n",
    "            str_to_save = \"[\" + str(label) + \"]:\\t\\t\" + data.loc[index][\"Key\"] + \"\\t\\t\" + str(\n",
    "                list_tuple_step_id[index][0]) + \"\\t\\t\" + str(list_test_step_clustering[index]) + \"\\n\"\n",
    "            output.write(str_to_save)\n",
    "    print(\"Successfully generated text file with title = clustered_data_kmeans.txt\")\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ry0wEguAh798",
    "outputId": "f5b9c1a4-40d3-4b1d-d133-132ee055c8e8"
   },
   "outputs": [],
   "source": [
    "def save_cluster_labels(labels, list_tuple_step_id):\n",
    "    \"\"\"\n",
    "    Saves cluster labels in a text-file\n",
    "\n",
    "    :param labels: k-means labels\n",
    "    :param list_tuple_step_id: tuple of step-id\n",
    "    \"\"\"\n",
    "    path = \"clustered_labels_kmeans.txt\"\n",
    "    output = open(path, \"a\")\n",
    "    for single_label in set(labels):\n",
    "        indices_label = np.where(labels == single_label)[0].tolist()\n",
    "        str_to_save = \"[\" + str(single_label) + \"]: \" + ','.join(\n",
    "            str(list_tuple_step_id[x][0]) for x in indices_label) + \"\\n\"\n",
    "        output.write(str_to_save)\n",
    "    print(\"Successfully generated text file with title = clustered_labels_kmeans.txt\")\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ij1lfQ4-iT7u"
   },
   "source": [
    "# Code Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "oGX1W4j9iNu2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----START----\n",
      "Reading input data\n",
      "Shape of data =  (369, 5)\n",
      "Size of dataset before preprocessing =  (369, 5)\n",
      "Number of unique words across Test-cases = 352 & Test-Steps = 818 after Tokenization\n",
      "Number of unique words across Test-cases = 310 & Test-Steps = 742 after Stopword Removal\n",
      "Number of words that occurred only once in Test-Cases = 16& Test-Steps = 331\n",
      "Unique word count in Test-Case = 281 & Unique word count in Test-Steps = 683\n",
      "Size of dataset after preprocessing =  (369, 5)\n",
      "Returning Training list\n",
      "Length of list with training data = 369\n",
      "Training word2vec model....\n",
      "Returning data tuple\n",
      "First Tuple = (1.0, ['enter', 'phone', 'number'])\n",
      "Initializing Similarity Matrix\n",
      "Computing Similarity using WMD and filling Similarity Matrix\n",
      "Number of clusters = 25 & Number of labels = [ 5 11 17  5 17  5 11  1  5 11 12 12  5 18  5 11 11 18  5 11 19 18  5 17\n",
      " 15 15  1  7  1  7 20  7  1  7  1  7 20  7 20  7  2  7 16 16 16  3  2  7\n",
      " 16 16 16  3  2  7 16 16 16  3 13 10 14 19 14 14 14 14 14 14  6 14 13 10\n",
      " 14 14 14 14  5 18 12  1  1  8 21 19 19 19 20 19 20 21 19  8  8  4  6  1\n",
      "  1 12  1  1  1  4  6  1  1  1 19 19 19 21 19  6  6  6 19 19 19 19  6  6\n",
      "  3 23  8 23 13  1  1  6  6  6  6  6  6  6  4 20  7 20  0 21  6  6  1  1\n",
      "  1  1  6  4  2  7 22  7  8  7  7  0  0 17  8 21 14  6  2  2  3  0 17  2\n",
      "  2  2  2 20  8  2 24 21  3 19 19 19 24  4 23  0  9 17  6  0  7  0  0 17\n",
      " 14  0  0 17 14  3  6  8  3  8  3 14  3  9  9  9  8  8 17  9 15 15  0 15\n",
      "  3 15 15 15 15 15  0 15  4 15  4 20 20  2  2  2  2  6  2  3  3  3  3 21\n",
      "  3 21  3  3  0  8  8  8  3 10 10 10  9  8 24 20 19 20 21  0  8 21  8 21\n",
      " 21  4 21 19 19 21  8  3 21  3 20  2  0 14  6  7  0  7 17 10 10 10 10  9\n",
      "  9  9  9  9  0  0  0  9  9  9  6  9  9  9 21  3  4  9  9 19 24  8 19  4\n",
      " 19 19  4  4 19  8 19  8 23 20  7 23  4 17  4  8 23 23  4 19  4  9  9  9\n",
      "  9  4 22  4  8 10  0  8 21 24  5 11  9  9  4  8 19 24  4  4 20 20  4  4\n",
      "  4  4 20  4  8  4  2  7 22]\n",
      "Successfully generated text file with title = clustered_data_kmeans.txt\n",
      "Successfully generated text file with title = clustered_labels_kmeans.txt\n",
      "----DONE----\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(\"----START----\")\n",
    "\n",
    "training_data = pd.read_excel(DATASET_PATH)\n",
    "\n",
    "# Preprocess the training dataset\n",
    "read_input_data(training_data)\n",
    "test_steps_df = clean_dataset(training_data)\n",
    "training_list = return_training_list(test_steps_df)\n",
    "\n",
    "# Loading pre-trained on 15GB of Stack Overflow posts Word2Vec Model\n",
    "pre_trained_model_path = \"SO_vectors_200.bin\"\n",
    "pre_trained_model = KeyedVectors.load_word2vec_format(\n",
    "    pre_trained_model_path,\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "# Train the word2vec model using the new dataset\n",
    "my_model = train_word2vec_model(\n",
    "    training_list,\n",
    "    pre_trained_model,\n",
    "    pre_trained_model_path,\n",
    "    VECTOR_DIMENSION,\n",
    "    WORKER_COUNT,\n",
    "    WORD_FREQUENCY_THRESHOLD,\n",
    "    CONTEXT_WINDOW_SIZE,\n",
    "    DOWN_SAMPLING\n",
    ")\n",
    "\n",
    "# Obtain tuple for step_id and list of test-step clustering from the training data\n",
    "list_tuple_step_id, list_test_step_clustering = return_data_tuple(test_steps_df)\n",
    "\n",
    "# Create an Empty matrix with rows and columns equal to the shape of 'list_test_step_clustering'\n",
    "matrix_similarity_distance = initialize_similarity_matrix(list_test_step_clustering)\n",
    "\n",
    "# Calculate similarity between word embeddings using Word-Movers Distance and save the result\n",
    "matrix_similarity_distance = compute_and_save_similarity_distance(\n",
    "    my_model,\n",
    "    list_test_step_clustering,\n",
    "    matrix_similarity_distance\n",
    ")\n",
    "\n",
    "# Perform Clustering of test-steps using K-Means\n",
    "dict_clusters = perform_clustering_and_save(\n",
    "    my_model,\n",
    "    pre_trained_model,\n",
    "    test_steps_df,\n",
    "    list_tuple_step_id,\n",
    "    list_test_step_clustering\n",
    ")\n",
    "\n",
    "print(\"----DONE----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
